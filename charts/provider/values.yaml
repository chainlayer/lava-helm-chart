## Lava Provider Configuration
## Ref: https://github.com/chainlayer/lava-helm-chart
##

# -- Provide a name in place of `lava-provider`
nameOverride: ""
# -- String to fully override `"common.fullname"`
fullnameOverride: ""
# -- Provide a namespace in place of release namespace
namespaceOverride: ""

# -- Configuration for the Lava Provider
## Ref: https://docs.lavanet.xyz/provider-setup
config:
  # -- Manage the provider with lavavisor
  ## Ref: https://docs.lavanet.xyz/lavavisor
  lavavisor: false
  # -- The log level of the provider
  logLevel: info
  # -- The moniker of the provider
  moniker: test
  # -- The public endpoint of the provider
  endpoint: ""
  # -- The chain ID for the provider
  chainId: ""
  # -- The node URL of a Lava node
  rpcNodeUrl: ""

  # -- Configure the private key and password for the provider
  ## Ref: https://docs.lavanet.xyz/wallet#cli
  key:
    # -- The name of the account that is being imported from the private key
    name: ""
    # -- The name of the secret containing the private key
    secretName: ""
    # -- The key in the secret containing the private key
    secretKey: ""
    # -- The name of the secret containing the private key password
    passwordSecretName: ""
    # -- The key in the secret containing the private key password
    passwordSecretKey: ""

  cache:
    # -- Use a cache service for the provider
    ## Requires the cache service to be deployed separately
    ## Ref: https://docs.lavanet.xyz/provider-features#caching
    enabled: false
    # -- The endpoint of the cache service
    endpoint: ""

  # -- Additional flags to pass to the provider process.
  ## Use the `lavap --help` command to see all available flags
  flags: {}
    # parallel-connections: 10
    # polling-multiplier: 1.5

  # -- The geolocation of the provider
  ## Ref: https://docs.lavanet.xyz/provider-setup#geolocation
  geolocation: ""

  # -- A list of supported chain IDs
  ## This list must match what is provided in the configuration file for the provider service
  ## Ref: https://docs.lavanet.xyz/provider-setup/#configuration-examples
  supportedChainIds: []
    # - LAV1

  # -- The backend configration for the provider service
  configYaml: {}
    # endpoints:
    #   - api-interface: tendermintrpc
    #     chain-id: LAV1
    #     network-address:
    #       address: 0.0.0.0:2024
    #       disable-tls: true
    #     node-urls:
    #       - url: http://lava-rpc.mydomain.com:26657
    #       - url: ws://lava-rpc.mydomain.com:26657/websocket
    #   - api-interface: rest
    #     chain-id: LAV1
    #     network-address:
    #       address: 0.0.0.0:2024
    #       disable-tls: true
    #     node-urls:
    #       - url: http://lava-rpc.mydomain.com:1317
    #   - api-interface: grpc
    #     chain-id: LAV1
    #     network-address:
    #       address: 0.0.0.0:2024
    #       disable-tls: true
    #     node-urls:
    #       - url: lava-rpc.mydomain.com:9090

  existingConfigMap:
    # -- Use an existing ConfigMap for the provider configuration
    enabled: false
    # -- The name of the existing ConfigMap
    name: ""
    # -- The key in the ConfigMap containing the configuration
    key: ""
    # -- The path to mount the configuration file to. If specified, it must end with ".yaml"
    path: ""

  # -- Environment variables to pass to the provider process
  env: []

  # -- Load additional environment variables from a secret or configmap
  envFrom: []

# -- Configuration for the Lava provider instance
provider:
  image:
    # -- The repository of the image to pull
    repository: ""
    # -- The tag of the image to pull
    tag: ""
    # -- The digest of the image to pull. Takes precedence over the tag
    digest: ""
    # -- The pull policy for the image. One of Always, Never, IfNotPresent
    pullPolicy: IfNotPresent
    # -- List of secrets to use for pulling images from private registries. Requires .serviceAccount.enabled to be true
    ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    pullSecrets: []

  # -- The number of replicas to deploy. This value is only used if autoscaling is disabled
  ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#scaling
  replicaCount: 1

  # -- The port used for the container
  containerPort:
    name: provider
    number: 2221

  # -- Additional ports to expose on the container
  extraPorts: []
  #  - name: eth1
  #    number: 2222
  #    protocol: TCP

  # -- Probe configuration for the pods
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/
  readinessProbe:
    # -- Enable Kubernetes liveness probe for the pods
    enabled: true
    # -- Number of seconds after the container has started before [probe] is initiated
    initialDelaySeconds: 15
    # -- How often (in seconds) to perform the [probe]
    periodSeconds: 10
    # -- Number of seconds after which the [probe] times out
    timeoutSeconds: 5
    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed
    successThreshold: 1
    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded
    failureThreshold: 5

  livenessProbe:
    # -- Enable Kubernetes liveness probe for the pods
    enabled: true
    # -- Number of seconds after the container has started before [probe] is initiated
    initialDelaySeconds: 30
    # -- How often (in seconds) to perform the [probe]
    periodSeconds: 5
    # -- Number of seconds after which the [probe] times out
    timeoutSeconds: 5
    # -- Minimum consecutive successes for the [probe] to be considered successful after having failed
    successThreshold: 1
    # -- Minimum consecutive failures for the [probe] to be considered failed after having succeeded
    failureThreshold: 10

  # -- Custom liveness probe configuration
  customLivenessProbe: {}

  # -- Custom readiness probe configuration
  customReadinessProbe: {}

  # -- Resource limits and requests for the pods
  ## Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 1500m
      memory: 2Gi

  # -- Configure the pod security context
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
  podSecurityContext:
    enabled: false
    # runAsUser: 1000
    # runAsGroup: 1000
    # fsGroup: 1000
    # runAsNonRoot: true

  # -- Configure the security context for the container
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-container
  containerSecurityContext:
    enabled: false
    # seLinuxOptions: {}
    # runAsUser: 1001
    # runAsGroup: 1001
    # runAsNonRoot: true
    # readOnlyRootFilesystem: true
    # privileged: false
    # allowPrivilegeEscalation: false
    # capabilities:
    #   drop: ["ALL"]
    # seccompProfile:
    #   type: "RuntimeDefault"

  # -- Additional labels to be applied to the Deployment or StatefulSet
  deploymentLabels: {}
    # my-deployment-label: value

  # -- Additional annotations to be applied to the Deployment or StatefulSet
  deploymentAnnotations: {}
    # my-deployment-annotation: value

  # -- Additional labels to be applied to the pods
  podLabels: {}
    # my-pod-label: value

  # -- Additional annotations to be applied to the pods
  podAnnotations: {}
    # my-pod-annotation: value

  # -- Specify constraints to limit the nodes the pods can be scheduled on
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity
  affinity: {}
    # requiredDuringSchedulingIgnoredDuringExecution:
    #   nodeSelectorTerms:
    #   - matchExpressions:
    #     - key: topology.kubernetes.io/zone
    #       operator: In
    #       values:
    #       - antarctica-east1
    #       - antarctica-west1
    # preferredDuringSchedulingIgnoredDuringExecution:
    # - weight: 1
    #   preference:
    #     matchExpressions:
    #     - key: another-node-label-key
    #       operator: In
    #       values:
    #       - another-node-label-value

  # -- Schedule the pods on specific nodes
  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/
  nodeSelector: {}
    # kubernetes.io/zone: eu-central

  # -- [Tolerations] for use with node taints
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
  tolerations: []
    # - key: "kubernetes.io/nodeType"
    #   operator: "Equal"
    #   value: "gpu"
    #   effect: "NoSchedule"

  # -- Specify the strategy used to replace old pods by new ones
  ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
  updateStrategy: {}
    # type: RollingUpdate
    # rollingUpdate:
    #   maxSurge: 25%
    #   maxUnavailable: 5%

  # -- Define the pod management policy for the StatefulSet
  ## Requires persistence.enabled: true
  ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#deployment-and-scaling-guarantees
  podManagementPolicy: OrderedReady

  # -- Configure the priority class for the pods
  # Ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
  priorityClassName: ""

  # -- Assign custom [TopologySpreadConstraints] rules to the pods
  # Ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
  topologySpreadConstraints: []
    # - maxSkew: 1
    #   topologyKey: kubernetes.io/hostname
    #   whenUnsatisfiable: DoNotSchedule
    #   labelSelector:
    #     matchLabels:
    #       app: foo
    #   matchLabelKeys:
    #     - pod-template-hash

  # -- Additional init containers to be added to the deployed pods
  initContainers: []

  # -- Additional sidecar containers to be added to the deployed pods
  sidecars: []

  # -- Configure a pod disruption budget for the pods
  ## Ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
  pdb:
    # -- Create a [PodDisruptionBudget] for the application controller
    enabled: false
    # -- Number of pods that are available after eviction as number (eg.: 1) or percentage (eg.: 50%)
    minAvailable: 1
    # -- Number of pods that are unavailable after eviction as number or percentage (eg.: 50%)
    maxUnavailable: 3

  # -- Autoscaling configuration for the pods
  ## Vertical Pod Autoscaler (VPA) and Horizontal Pod Autoscaler (HPA) should not be enabled at the same time with the same resource
  autoscaling:
    vpa:
      # -- Enable a Vertical Pod Autoscaler
      ## Ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler
      enabled: false
      # -- Additional annotations to apply to the VPA
      annotations: {}
      # -- Resources to be controlled by the VPA
      controlledResources: []
      # -- The minimum number of replicas
      minAllowed: 1
      # -- The maximum number of replicas
      maxAllowed: 3
      # -- Update policy for the VPA
      updatePolicy:
        updateMode: Auto

    hpa:
      # -- Enable a Horizontal Pod Autoscaler
      ## Ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
      enabled: false
      # -- Additional annotations to apply to the HPA
      annotations: {}
      # -- The minimum number of replicas
      minReplicas: 1
      # -- The maximum number of replicas
      maxReplicas: 5
      # -- The target CPU or Memory utilization percentage
      targetCPU: "70"
      targetMemory: "70"
      # -- Custom metrics to use for scaling
      customTargets: []
        # - name: "my-metric"
        #   type: "Object"
        #   object:
        #     metricName: "my-metric"
        #     target: "50"

service:
  # -- Enable a service resource for the pods
  enabled: true
  # -- The service name override. If not set, the name is generated using the fullname template
  nameOverride: ""
  # -- The type of service to create
  type: ClusterIP
  # -- The port used for the service
  port:
    # -- The port name. If not set, the containerPort.name is used
    name: ""
    # -- The port number, If not set, the containerPort.number is used
    number: ""
  # -- ClusterIP will get created with the IP specified in this field
  clusterIP: ""
  # -- LoadBalancer will get created with the IP specified in this field
  loadBalancerIP: ""
  # -- Source IP ranges to allow access to service from
  loadBalancerSourceRanges: []
  # -- Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
  externalTrafficPolicy: Cluster
  # -- Additional annotations to apply to the service
  annotations: {}
  # -- Used to maintain session affinity. Supports `ClientIP` and `None`
  sessionAffinity: None
  # -- Additional session affinity configuration
  sessionAffinityConfig: {}
    # sessionAffinityConfig:
    # clientIP:
    #   timeoutSeconds: 600

ingress:
  # -- Enable an ingress resource for the pods
  enabled: false
  # -- Define which ingress class will implement the ingress
  className: ""
  # -- Additional annotations to apply to the ingress
  annotations: {}
    # cert-manager.io/cluster-issuer: letsencrypt-prod
    # nginx.ingress.kubernetes.io/backend-protocol: "GRPC"
  # -- A list of hostnames to be covered by ingress record
  hosts: []
    # - host: provider.mydomain.com
    #   paths:
    #     - path: /
    #       pathType: ImplementationSpecific
  # -- Configure TLS for the ingress with secrets
  tls: []
  #  - secretName: provider-mydomain-com-tls
  #    hosts:
  #      - provider.mydomain.com

persistence:
  # -- Create a PVC for the pods
  enabled: false
  # -- The path to mount the volume to
  mountPath: /data
  # -- The sub path to mount the volume to
  subPath: ""
  # -- Additional annotations to apply to the PVC
  annotations: {}
  # -- The storage class to use
  storageClass: ""
  # -- Access mode for the PVC
  accessModes:
    - ReadWriteOnce
  # -- The storage claim size for the PVC
  size: ""
  # -- The name of an existing PVC to use
  # When set, a new PVC is not created
  existingClaim: ""
  # -- Selector applied to the PVC
  selector: {}
  # -- Define the volume snapshot or existing PVC to restore from
  dataSource: {}
    # kind: PersistentVolumeClaim
    # name: my-pvc

serviceAccount:
  # -- Create a service account for the pods
  enabled: true
  # -- Service account name override. If not set, the name is generated using the fullname template
  nameOverride: ""
  # -- Additional annotations to apply to created service account
  annotations: {}
  # -- Automount credentials for the service account into the pod.
  automountServiceAccountToken: true

metrics:
  # -- Enable and expose metrics
  enabled: false
  port:
    # -- Metrics service port name
    name: metrics
    # -- Metrics service port number
    number: 7776

  serviceMonitor:
    # -- Enable a Prometheus ServiceMonitor
    enabled: false
    # -- The namespace to deploy the ServiceMonitor. If not set, the namespace of the release is used
    namespace: "" # "monitoring"
    # -- Additional labels to apply to the ServiceMonitor
    labels: {}
    # -- Additional annotations to apply to the ServiceMonitor
    annotations: {}
    # -- Service label for use in assembling a job name of the form <label value>-<port>. If no label is specified, the service name is used
    jobLabel: ""
    # -- Keep labels from scraped data, overriding server-side labels
    honorLabels: false
    # -- Define the interval at which metrics are scraped
    interval: ""
    # -- Define how long to wait before considering a scrape request as failed
    scrapeTimeout: ""
    # -- The path for the metrics service
    path: "/metrics"
    # -- MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    metricRelabelings: []
    # -- RelabelConfigs to apply to samples before scraping
    relabelings: []
    # -- Label selector for services to which this ServiceMonitor applies
    selector: {}
      # label: value

  rules:
    # -- Deploy a PrometheusRule for the application controller
    enabled: false
    # -- Specify the namespace to deploy the PrometheusRule. If not set, the namespace of the release is used
    namespace: ""
    # -- Specify selector(s) to target Prometheus instances
    selector: {}
      # prometheus: kube-prometheus
    # -- Additional labels to apply to the PrometheusRule
    labels: {}
    # -- Additional annotations to apply to the PrometheusRule
    annotations: {}
    # -- Define the rules to be applied
    spec: []
      # - name: "LavaProviderAlerts"
      #   rules:
      #     - alert: "LavaProviderStoppedRelaying"
      #       expr: "sum(increase(lava_provider_total_relays_serviced[10m])) by(spec) == 0"
      #       for: "5m"
      #       labels:
      #         severity: "critical"
      #       annotations:
      #         description: "{{ $labels.spec }} has not serviced any relays for the past 15 minutes"
